**Writeup for Project**
---

**Vehicle Detection Project**

**The goals / steps of this project are the following:**

* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier
* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector.
* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.
* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.
* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.
* Estimate a bounding box for vehicles detected.
Ã‚
[//]: # (Image References)

[image1]: ./output_images/1.image_with_boxes_out.png "image_with_boxes_out"
[image2]: ./output_images/3.car_and_hog_Out.png "car_and_hog_Out"
[image3]: /output_images/2.car_and_not_a_car_Out.png "car_and_not_a_car_Out"
[image4]: ./output_images/4.non_car_and_hog_Out.png "non_car_and_hog_Out"
[image5]: ./output_images/test1_out.png "test1_out"
[image6]: ./output_images/test2_out.png "test2_out"
[image7]: ./output_images/test3_out.png "test3_out"

[image8]: ./output_video/image_snippets_for_video/testt0.png "vid_images1"
[image9]: ./output_video/image_snippets_for_video/testt7.png "vid_images2"
[image10]: ./output_video/image_snippets_for_video/testt13.png "vid_images3"
[image11]: ./output_video/image_snippets_for_video/testt21.png "vid_images4"
[image12]: ./output_video/image_snippets_for_video/testt25.png "vid_images5"
[image13]: ./output_video/image_snippets_for_video/testt29.png "vid_images6"

[image14]: ./output_images/4.random_car_and_features_plot.png "random_car_and_features_plot4"
[image15]: ./output_images/5.random_car_and_features_plot.png "random_car_and_features_plot5"





**Writeup / README**

**0. Create a helper fn. to create rectangle boxes based on coordinates/bounding boxes passed-in.**

In 3rd code cell of IPython notebook, helper function do_draw_boxes() is used as part of the notebook [Vehicle_Detection.ipynb](Vehicle_Detection.ipynb),

Input: Image and list of bounding rectangle coordinates
Output: Uses the OpenCV function cv2.rectangle() to draw the bounding rectangles on the input image.

Example:

![alt text][image1]

**Histogram of Oriented Gradients (HOG)**

**1. Explain how (and identify where in your code) you extracted HOG features from the training images.**

The fn. retrive_hog_features() in 9th code cell of IPython notebook [Vehicle_Detection.ipynb](Vehicle_Detection.ipynb), contains the code for extracting Histogram of Oriented features from an image. It uses the OpenCV function **hogDescriptor()**.

![alt text][image2]

![alt text][image4]

`vehicle` and `non-vehicle` images where taken as input.  One image of each of the `vehicle` and `non-vehicle` classes is listed here:

 ![alt text][image3]  


Different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`) were explored.  Images from randomly taken from 'vehicle' and 'non vehicle' classes and output for `skimage.hog()` was displayed.

In order to build training datasets by extracting HOG features, function **extract_image_features()** was definedin 12th code cell.

- The function takes and appends the HOG features to the dataset of training features for each image.
- Built the datasets of **vehicle** and **non-vehicle** images and labelled it appropriately.
- I also normalized the features to have 0 mean and unit variance by using function StandardScaler (from Scikit-learn)
- Training and testing datasets were splitusing 'train_test_split()' (from sklearn.model_selection).  

The total HOG features are :  **(1764,)**

Example:

![alt text][image14]

![alt text][image15]

**2. Explain how you settled on your final choice of HOG parameters.**

I tested the performance of the HOG features from Lab, HSV, YUV, and LUV color spaces and found the **YUV** color space to have the best performance for identifying vehicles. Therefore, **YUV** space for feature extraction.

**3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).**

I tested :
- Linear Support Vector Machine
- Logistic Regression Classifier
- Multi-layer Perceptron

In order to train the classifier to be used to identify vehicles in images. The performance evaluation of classifiers is as under:

| Classifier  | Training Accuracy  | Test Accuracy  | Prediction Time  |
|---|---|---|---|
|  LinearSVC |     1.0 | 0.95411  |  0.00034 |
|  Logistic Regression |   0.997|  0.95889 |  0.00017 |
|  Multi-layer Perceptron |  1.0 |     0.99183 |   0.00313|


Per above results,  **Multi-layer Perceptron** was chosen to identify vehicles in images and in the video stream.
- Between the **Logistic Regression Classifier** and  **Multi-layer Perceptron**, **Multi-layer Perceptron**,  MLP classifier was chosendue to the increase in prediction accuracy.

**Sliding Window Search**

**1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?**

The 21st code cell of IPython notebook [Vehicle_Detection.ipynb](Vehicle_Detection.ipynb) contains the sliding_window() fn.

* In oder to search vehicle(s) in images, sliding window approach was implemented by taking one slice of the image at a time
* Slide was used to make prediction on the HOG features.
* I searched for vehicles(s) only in the lower half of the image, in order to minimize the search area and speed up the pipeline.
* Searches for vehicles in windows of multiple scales (with an 80% overlap), in order to identify vehicles which can be near or far in the image and thus will appear as different sizes.
* Used MLP classifier's  predict_proba() fn., in order to ensure a high confidence for predictions, and used the probability score (picked score higher than 0.99) for each possible class.
* The window coordinates classified as vehicle are appended to a detected_windows[] list and finally calls fn. do_draw_boxes() to draw the boxes across the detected vehicles.
* cv2.findContours() was used to find all of the objects in the mask image to find contours
* cv2.boundingRect() is used on each contour to find the coordinates of the bounding rect for each vehicle.
* At the end, I created a copy of the original image, to draw the bounding rectangles.

Example:


![alt text][image5]

![alt text][image6]

![alt text][image7]

2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?

As above.

---

**Video Implementation**

**1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)**
The input video is at Udacity's  [link](https://github.com/udacity/CarND-Vehicle-Detection/blob/master/project_video.mp4)

Here's a [link to my video result](https://github.com/sid-arora/CarND-Vehicle-Detection/blob/master/output_video/project_video_ouput.mp4)


**2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.**

- In order to track vehicles across frames in a video stream, I created a class boxes to store all of the bounding rectangles from each of the previous 12 frames in a list.
- In each frame, I then combine the lists of bounding rectangles from current and previous frames, and then used the OpenCV function cv2.groupRectangles to combine overlapping rectangles in to consolidated bounding boxes.
- Within the group rectangles function I set the parameter groupThreshold equal to 10 which means that it will only look for places where there are greater than 10 overlapping boxes and it will ignore everything else.
- The group rectangles function takes care of the problem of false positives because if any part of the image is classified as a vehicle in fewer than 10 out of 12 consecutive frames, it will be filtered out and will not be included in the final bounding rectangles which are annotated on to the output video.


**Here are video's six frames, their corresponding heatmaps and annotataed image:**

![alt text][image8]

![alt text][image9]

![alt text][image10]

![alt text][image11]

![alt text][image12]

![alt text][image13]


---

**Discussion**

**1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?**

* Elimination of false positives for detected vehicle was the most difficult part of the project, as wrong detection can wrongly influence the car to make wrong driving decisions.

* The pipeline is still falsely identifying some non-vehicle objects such as trees and traffic signs. To improve the performance, it will be necessary to train on bigger dataset and add more negative features based on the false positives which were identified in the video stream.

Using OpenCV's HOG function from the skimage version greatly improved performance, but it is still not the best. Currently, my pipeline processes videos at about 4 frames per second, and to be useful in real time it would need to be closer to 25 or 30.

 * In an attempt to speed up the prediction process, it would help to to prune the number of features and reduce the number of windows searched 